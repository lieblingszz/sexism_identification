{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64eec80",
   "metadata": {
    "id": "jSWVnSUby109"
   },
   "source": [
    "# Install packages and download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8923b71",
   "metadata": {
    "id": "_Ww5hM6kE6XU"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/google-bert/bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c310fd0",
   "metadata": {
    "id": "b2wxdcL3GDnX"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf5ea0",
   "metadata": {
    "id": "XeJ3oni2SMfA"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04563570",
   "metadata": {
    "id": "lMB8Rq_hWFnx"
   },
   "source": [
    "# Build a new dataset with description and reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8f3cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3572,
     "status": "ok",
     "timestamp": 1714996714271,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "1L9FAnSOWNrk",
    "outputId": "b5fcb1c4-67d4-4959-f630-cfd39e068b29"
   },
   "outputs": [],
   "source": [
    "# Download EXIST dataset\n",
    "# Please manually download and extract the EXIST_dataset.zip to ./data/ directory\n",
    "# The dataset should contain GPT descriptions and initial answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ffedc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48294,
     "status": "ok",
     "timestamp": 1714996762564,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "4CAmn_VuaCnz",
    "outputId": "c20f580a-ffaa-4c53-8cee-4bc54c1cf220"
   },
   "outputs": [],
   "source": [
    "# Download EXIST 2024 Memes Dataset\n",
    "# Please manually download and extract the EXIST_2024_Memes_Dataset.zip to ./data/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a845b",
   "metadata": {
    "id": "kn9EpwNwaRdm"
   },
   "outputs": [],
   "source": [
    "original_training_path = './data/EXIST 2024 Memes Dataset/training/EXIST2024_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9744658",
   "metadata": {
    "id": "f5G6GGYGadJX"
   },
   "outputs": [],
   "source": [
    "GPT_english_path = './data/EXIST_dataset/English_Meme_GPT_Description_training.json'\n",
    "GPT_spanish_path = './data/EXIST_dataset/Spanish_Meme_GPT_Description_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3878fd0",
   "metadata": {
    "id": "xX4ik705apoh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_datasets(input_path_en, input_path_sp, input_path_original, output_path):\n",
    "    with open(input_path_en, 'r') as file:\n",
    "        data_en = json.load(file)\n",
    "    with open(input_path_sp, 'r') as file:\n",
    "        data_sp = json.load(file)\n",
    "    with open(input_path_original, 'r') as file:\n",
    "        data_original = json.load(file)\n",
    "\n",
    "    for key, value in data_en.items():\n",
    "        image_id = key.split('.')[0]\n",
    "        if image_id in data_original:\n",
    "            data_original[image_id]['description'] = value['description']\n",
    "            data_original[image_id]['reference'] = value['reference']\n",
    "        else:\n",
    "            print(f'EN ID {image_id} not found in the original file.')\n",
    "\n",
    "    for key, value in data_sp.items():\n",
    "        image_id = key.split('.')[0]\n",
    "        if image_id in data_original:\n",
    "            data_original[image_id]['description'] = value['description']\n",
    "            data_original[image_id]['reference'] = value['reference']\n",
    "        else:\n",
    "            print(f'SP ID {image_id} not found in the original file.')\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(data_original, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Data has been merged and saved in\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4577e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1714996763102,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "aiiCAuBwmOSw",
    "outputId": "c519e9bc-c618-43d1-e5a8-8314d786bebb"
   },
   "outputs": [],
   "source": [
    "build_datasets(GPT_english_path, GPT_spanish_path, original_training_path, \"./data/new_dataset_training.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31c362",
   "metadata": {
    "id": "X2DGhaTw9YRi"
   },
   "source": [
    "# Split english memes and spanish memes to training and test dataset seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df6e3a",
   "metadata": {
    "id": "yMvi0dq-65sB"
   },
   "outputs": [],
   "source": [
    "# Split the json file to english and spanish seperately\n",
    "def split_json_by_key(input_path, output_path_spanish, output_path_english):\n",
    "    # 读取原始JSON文件\n",
    "    with open(input_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 初始化两个字典来存储分类后的数据\n",
    "    spanish_memes = {}\n",
    "    english_memes = {}\n",
    "\n",
    "    # 遍历原始数据，根据key的首位数字进行分类\n",
    "    for key, value in data.items():\n",
    "        if key.startswith('1'):\n",
    "            spanish_memes[key] = value\n",
    "        elif key.startswith('2'):\n",
    "            english_memes[key] = value\n",
    "\n",
    "    # 写入Spanish memes数据到新的JSON文件\n",
    "    with open(output_path_spanish, 'w', encoding='utf-8') as file:\n",
    "        json.dump(spanish_memes, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 写入English memes数据到新的JSON文件\n",
    "    with open(output_path_english, 'w', encoding='utf-8') as file:\n",
    "        json.dump(english_memes, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Data has been successfully split and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f3f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1714996969669,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "S-BKoXu88GqO",
    "outputId": "6c55bcfb-b8ac-4d9e-a8f0-1d1b66c467a3"
   },
   "outputs": [],
   "source": [
    "split_json_by_key('./data/new_dataset_training.json', './data/spanish_memes.json', './data/english_memes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2223f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1872,
     "status": "ok",
     "timestamp": 1714996971540,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "x7gJR6Y4DLDo",
    "outputId": "edc08f4f-8e94-48cb-fa3e-22863ffe95f0"
   },
   "outputs": [],
   "source": [
    "# Download ground truth data\n",
    "# Please manually download EXIST2024_training_task4_gold_hard.json to ./data/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e23b7",
   "metadata": {
    "id": "GLSuqJG59g2t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# 将gold数据和现有的数据结合\n",
    "def process_data(main_json_path, ground_truth_json_path, test_size=0.2):\n",
    "    # 从文件中读取JSON数据\n",
    "    with open(main_json_path, 'r') as file:\n",
    "        main_data = json.load(file)\n",
    "    with open(ground_truth_json_path, 'r') as file:\n",
    "        ground_truth_data = json.load(file)\n",
    "\n",
    "    # 转换列表中的字典为DataFrame\n",
    "    main_df = pd.DataFrame.from_dict(main_data, orient='index')\n",
    "    ground_truth_df = pd.DataFrame(ground_truth_data)\n",
    "\n",
    "    # 假设ground_truth_df的数据已正确设置id为索引\n",
    "    ground_truth_df.set_index('id', inplace=True)\n",
    "    ground_truth_df.rename(columns={'value': 'gold'}, inplace=True)\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    complete_df = main_df.join(ground_truth_df, how='left', rsuffix='_gt')\n",
    "\n",
    "    # 返回处理后的训练集和测试集\n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d9554",
   "metadata": {
    "id": "-YrOyC5yDlLe"
   },
   "outputs": [],
   "source": [
    "# 调用这个函数处理数据\n",
    "# 首先分开处理english和spanish数据集\n",
    "spanish_df = process_data('./data/spanish_memes.json', './data/EXIST2024_training_task4_gold_hard.json')\n",
    "english_df = process_data('./data/english_memes.json', './data/EXIST2024_training_task4_gold_hard.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a459a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1714997119182,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "aplTV36oyzmm",
    "outputId": "a8b71515-cb76-4c20-a9d3-26134208be16"
   },
   "outputs": [],
   "source": [
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69854a42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1714751326058,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "-0vtZK4Z_WY7",
    "outputId": "d693b373-4e28-47f4-e0b5-eb995aff5ecc"
   },
   "outputs": [],
   "source": [
    "spanish_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcd308",
   "metadata": {
    "id": "2NC56NAnLNeG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def clean_df(df):\n",
    "\n",
    "    # 映射 'gold' 列并立即删除任何未映射的值（即映射之外的值导致的 NaN）\n",
    "    label_mapping = {'YES': 1, 'NO': 0}\n",
    "    df['gold'] = df['gold'].map(label_mapping, na_action='ignore')  # 使用 na_action='ignore' 保留 NaNs 以便于检测\n",
    "    df.dropna(subset=['gold'], inplace=True)  # 删除映射后仍为 NaN 的行\n",
    "\n",
    "    # 确保 text 和 description 列为字符串类型\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['description'] = df['description'].astype(str)\n",
    "\n",
    "    # 合并 'text' 和 'description' 列\n",
    "    df['combined_text'] = df['text'] + \" \" + df['description']\n",
    "\n",
    "    # 提取特征和标签\n",
    "    X = df['combined_text'].values\n",
    "    y = df['gold'].values.astype(int)  # 转换为整数\n",
    "    # # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47a505",
   "metadata": {
    "id": "zTeInFU988ld"
   },
   "source": [
    "# Fine-tuning using mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cd1e1",
   "metadata": {
    "id": "F1hiYhUgF1ar"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30d732",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1714751326424,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "Bei8ZZZRh2sb",
    "outputId": "9168cb52-b58e-4cd2-e71c-a3f075e58603"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    f1_result = f1_metric.compute(predictions=predictions, references=labels, average='binary')\n",
    "\n",
    "    accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_result['f1'],\n",
    "        \"accuracy\": accuracy_result['accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd65f9b",
   "metadata": {
    "id": "p7Dsvyi478Lh"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b389cc",
   "metadata": {
    "id": "M2-9wSTKqmhU"
   },
   "outputs": [],
   "source": [
    "X_train_spanish,X_test_spanish, y_train_spanish, y_test_spanish = clean_df(spanish_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a587a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1714998548292,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "zcu946dE4QZJ",
    "outputId": "073e7137-51ca-4da1-98ce-3c1d19b0d09c"
   },
   "outputs": [],
   "source": [
    "print(\"Number of entries in X_train_spanish:\", len(X_train_spanish))\n",
    "print(\"Number of entries in X_test_spanish:\", len(X_test_spanish))\n",
    "print(\"Number of entries in y_train_spanish:\", len(y_train_spanish))\n",
    "print(\"Number of entries in y_test_spanish:\", len(y_test_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390268e",
   "metadata": {
    "id": "LXVidcMlqytt"
   },
   "outputs": [],
   "source": [
    "X_train_english, X_test_english, y_train_english, y_test_english = clean_df(english_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a855e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1714998508677,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "YTDHo5pY4Gt8",
    "outputId": "a67ed00c-b5de-413f-ca1c-0a8d337c0ccf"
   },
   "outputs": [],
   "source": [
    "print(\"Number of entries in X_train_english:\", len(X_train_english))\n",
    "print(\"Number of entries in X_test_english:\", len(X_test_english))\n",
    "print(\"Number of entries in y_train_english:\", len(y_train_english))\n",
    "print(\"Number of entries in y_test_english:\", len(y_test_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5fdd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1714998700022,
     "user": {
      "displayName": "Rong",
      "userId": "15769398465651090791"
     },
     "user_tz": -120
    },
    "id": "dbmf9CIs4JMh",
    "outputId": "969f4c30-268d-4cd7-ecc7-6293ed3b3de5"
   },
   "outputs": [],
   "source": [
    "# Concatenate training features (texts) from both languages\n",
    "X_train_combined = np.concatenate((X_train_spanish, X_train_english))\n",
    "X_test_combined = np.concatenate((X_test_spanish, X_test_english))\n",
    "\n",
    "# Concatenate training labels from both languages\n",
    "y_train_combined = np.concatenate((y_train_spanish, y_train_english))\n",
    "y_test_combined = np.concatenate((y_test_spanish, y_test_english))\n",
    "\n",
    "print(\"Combined X_train size:\", len(X_train_combined))\n",
    "print(\"Combined X_test size:\", len(X_test_combined))\n",
    "print(\"Combined y_train size:\", len(y_train_combined))\n",
    "print(\"Combined y_test size:\", len(y_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39bb28",
   "metadata": {
    "id": "b6z0pnEmz8Zu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt(f'./data/english_train_X.txt', X_train_english, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/english_test_X.txt', X_test_english, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/english_train_y.txt', y_train_english, fmt='%d')\n",
    "np.savetxt(f'./data/english_test_y.txt', y_test_english, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf0d9f",
   "metadata": {
    "id": "xxdZ6WLd1B6l"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'./data/spanish_train_X.txt', X_train_spanish, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/spanish_test_X.txt', X_test_spanish, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/spanish_train_y.txt', y_train_spanish, fmt='%d')\n",
    "np.savetxt(f'./data/spanish_test_y.txt', y_test_spanish, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c336a9",
   "metadata": {
    "id": "tLCwqKXl1QxU"
   },
   "outputs": [],
   "source": [
    "np.savetxt(f'./data/combined_train_X.txt', X_train_combined, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/combined_test_X.txt', X_test_combined, fmt='%s', encoding='utf-8')\n",
    "np.savetxt(f'./data/combined_train_y.txt', y_train_combined, fmt='%d')\n",
    "np.savetxt(f'./data/combined_test_y.txt', y_test_combined, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78048e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1714751327455,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "IDkQPXIcm9m3",
    "outputId": "3e706056-fc9d-483f-94d5-60d693391600"
   },
   "outputs": [],
   "source": [
    "tokenizer_mbert = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model_mbert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
    "model_mbert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0251e40",
   "metadata": {
    "id": "MrpvTzwulNEB"
   },
   "source": [
    "### Fine-tune on the Spanish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793691a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "executionInfo": {
     "elapsed": 188273,
     "status": "ok",
     "timestamp": 1714751516070,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "0_ztNlh3lGbs",
    "outputId": "e5e5483c-1b17-4955-812f-c91232aa6c20"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train_spanish, y_train_spanish, tokenizer_mbert)\n",
    "test_dataset = TextDataset(X_test_spanish, y_test_spanish, tokenizer_mbert)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_mbert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a6fb9",
   "metadata": {
    "id": "qqSzRHU6leha"
   },
   "source": [
    "### Fine-tune on the English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d341c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "executionInfo": {
     "elapsed": 195164,
     "status": "ok",
     "timestamp": 1714751711223,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "TcFin78KlDhd",
    "outputId": "ba9ab98b-008a-48f8-dad1-af8c1ef62fe8"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train_english, y_train_english, tokenizer_mbert)\n",
    "test_dataset = TextDataset(X_test_english, y_test_english, tokenizer_mbert)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_mbert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ef802",
   "metadata": {
    "id": "RvBtMTUrmv2A"
   },
   "source": [
    "# Fine-tune using XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991b019",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2783,
     "status": "ok",
     "timestamp": 1714751713988,
     "user": {
      "displayName": "Jing Ma",
      "userId": "06301856347973252019"
     },
     "user_tz": -120
    },
    "id": "c09O6TV-8ws4",
    "outputId": "3b48f4b0-8a46-421f-92ef-61b22dfbb8b9"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "tokenizer_xlm = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model_xlm = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)\n",
    "model_xlm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ffc3e",
   "metadata": {
    "id": "XKTDXNZMnGNf"
   },
   "source": [
    "### Fine-tune on Spanish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84bb5a",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "nZ8kscPVnPC7",
    "outputId": "098577f4-53be-4942-a520-9937dc26eccd"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train_spanish, y_train_spanish, tokenizer_xlm)\n",
    "test_dataset = TextDataset(X_test_spanish, y_test_spanish, tokenizer_xlm)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_xlm,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69326d76",
   "metadata": {
    "id": "uOyVlcsSnJwX"
   },
   "source": [
    "### Fine-tune on English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b61176",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XMqyQvaRnSVh",
    "outputId": "8063fc2e-1f07-4e24-e17e-95d0e7b22f4f"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train_english, y_train_english, tokenizer_xlm)\n",
    "test_dataset = TextDataset(X_test_english, y_test_english, tokenizer_xlm)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1.0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_xlm,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jSWVnSUby109",
    "X2DGhaTw9YRi"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
